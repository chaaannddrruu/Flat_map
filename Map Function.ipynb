{"cells":[{"cell_type":"code","source":["from pyspark import SparkConf, SparkContext\nconf = SparkConf().setAppName(\"hmc\")\nsc = SparkContext.getOrCreate(conf = conf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c7ea437-2a87-4619-94c8-2a23a0d23ef2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = sc.textFile('/FileStore/tables/new_3.txt')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4810b72-f1df-4bd9-b4aa-346d5293b7c8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5086d8a1-bba3-4988-8118-c25d7e059f01"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[6]: ['1)Self intro',\n '2)what are the transformations and where did u used these transformations that u have used in the project',\n '3)any agregations u have done in your project?',\n '4)have you worked on any pyspark performance tuning jobs?',\n '5)what was the purpose of doing partition?',\n '6)Why partition is it due to skewness?',\n '7)have u used repartition and Coalesce?',\n '8)have you used AWS only for storage or analytical?',\n '9)Redshift is used for sql quires like datawarehouse?',\n '',\n '',\n '1)what is difference between hospitalaty and hospital',\n '2)what is source of data and destination of data keeping',\n '3)how to read data in spark?',\n '4)how to read data in s3 to in python file',\n '5)can you write spark session',\n '6)what is transformation you performed? and write syntax',\n '7)how will you do transformation in dataframe ?',\n '8)while map transormations using how you print paritcular column?',\n '9)how will do map function in rdd and dataframe explain?',\n '10)how you used map functions in your project?',\n '11)why rdd instead of dataframe using explain in your project',\n '12)write sql quire using emp_id,first_name,last_name,manager_id,salary, whose salary is greater than the manager salary?']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[6]: ['1)Self intro',\n '2)what are the transformations and where did u used these transformations that u have used in the project',\n '3)any agregations u have done in your project?',\n '4)have you worked on any pyspark performance tuning jobs?',\n '5)what was the purpose of doing partition?',\n '6)Why partition is it due to skewness?',\n '7)have u used repartition and Coalesce?',\n '8)have you used AWS only for storage or analytical?',\n '9)Redshift is used for sql quires like datawarehouse?',\n '',\n '',\n '1)what is difference between hospitalaty and hospital',\n '2)what is source of data and destination of data keeping',\n '3)how to read data in spark?',\n '4)how to read data in s3 to in python file',\n '5)can you write spark session',\n '6)what is transformation you performed? and write syntax',\n '7)how will you do transformation in dataframe ?',\n '8)while map transormations using how you print paritcular column?',\n '9)how will do map function in rdd and dataframe explain?',\n '10)how you used map functions in your project?',\n '11)why rdd instead of dataframe using explain in your project',\n '12)write sql quire using emp_id,first_name,last_name,manager_id,salary, whose salary is greater than the manager salary?']"]}}],"execution_count":0},{"cell_type":"code","source":["def len_str(x):\n    l = x.split('\\n')\n    l2 = []\n    for i in l:\n        l2.append(len(i))\n    return l2\n\ndf2 = df.map(len_str)\ndf2.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b9805c2-9f9a-4ec4-a130-587d1cdbf5ce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: [[12],\n [105],\n [46],\n [57],\n [42],\n [38],\n [39],\n [51],\n [53],\n [0],\n [0],\n [53],\n [56],\n [28],\n [42],\n [29],\n [56],\n [47],\n [65],\n [56],\n [46],\n [61],\n [120]]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: [[12],\n [105],\n [46],\n [57],\n [42],\n [38],\n [39],\n [51],\n [53],\n [0],\n [0],\n [53],\n [56],\n [28],\n [42],\n [29],\n [56],\n [47],\n [65],\n [56],\n [46],\n [61],\n [120]]"]}}],"execution_count":0},{"cell_type":"code","source":["df3 = df.map(lambda x: [len(i) for i in x.split(' ')])\ndf3.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b9db035-6ea0-4562-90d9-66dc75dc5c27"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: [[6, 5],\n [6, 3, 3, 15, 3, 5, 3, 1, 4, 5, 15, 4, 1, 4, 4, 2, 3, 7],\n [5, 11, 1, 4, 4, 2, 4, 8],\n [6, 3, 6, 2, 3, 7, 11, 6, 5],\n [6, 3, 3, 7, 2, 5, 10],\n [5, 9, 2, 2, 3, 2, 9],\n [6, 1, 4, 11, 3, 9],\n [6, 3, 4, 3, 4, 3, 7, 2, 11],\n [10, 2, 4, 3, 3, 6, 4, 14],\n [0],\n [0],\n [6, 2, 10, 7, 11, 3, 8],\n [6, 2, 6, 2, 4, 3, 11, 2, 4, 7],\n [5, 2, 4, 4, 2, 6],\n [5, 2, 4, 4, 2, 2, 2, 2, 6, 4],\n [5, 3, 5, 5, 7],\n [6, 2, 14, 3, 10, 3, 5, 6],\n [5, 4, 3, 2, 14, 2, 9, 1],\n [7, 3, 14, 5, 3, 3, 5, 10, 7],\n [5, 4, 2, 3, 8, 2, 3, 3, 9, 8],\n [6, 3, 4, 3, 9, 2, 4, 8],\n [6, 3, 7, 2, 9, 5, 7, 2, 4, 7],\n [8, 3, 5, 5, 46, 5, 6, 2, 7, 4, 3, 7, 7]]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: [[6, 5],\n [6, 3, 3, 15, 3, 5, 3, 1, 4, 5, 15, 4, 1, 4, 4, 2, 3, 7],\n [5, 11, 1, 4, 4, 2, 4, 8],\n [6, 3, 6, 2, 3, 7, 11, 6, 5],\n [6, 3, 3, 7, 2, 5, 10],\n [5, 9, 2, 2, 3, 2, 9],\n [6, 1, 4, 11, 3, 9],\n [6, 3, 4, 3, 4, 3, 7, 2, 11],\n [10, 2, 4, 3, 3, 6, 4, 14],\n [0],\n [0],\n [6, 2, 10, 7, 11, 3, 8],\n [6, 2, 6, 2, 4, 3, 11, 2, 4, 7],\n [5, 2, 4, 4, 2, 6],\n [5, 2, 4, 4, 2, 2, 2, 2, 6, 4],\n [5, 3, 5, 5, 7],\n [6, 2, 14, 3, 10, 3, 5, 6],\n [5, 4, 3, 2, 14, 2, 9, 1],\n [7, 3, 14, 5, 3, 3, 5, 10, 7],\n [5, 4, 2, 3, 8, 2, 3, 3, 9, 8],\n [6, 3, 4, 3, 9, 2, 4, 8],\n [6, 3, 7, 2, 9, 5, 7, 2, 4, 7],\n [8, 3, 5, 5, 46, 5, 6, 2, 7, 4, 3, 7, 7]]"]}}],"execution_count":0},{"cell_type":"code","source":["mapped = df.map(lambda x: x.split(\" \"))\nmapped.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b904e62e-5b16-4581-8d95-066c39dd987b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[9]: [['1)Self', 'intro'],\n ['2)what',\n  'are',\n  'the',\n  'transformations',\n  'and',\n  'where',\n  'did',\n  'u',\n  'used',\n  'these',\n  'transformations',\n  'that',\n  'u',\n  'have',\n  'used',\n  'in',\n  'the',\n  'project'],\n ['3)any', 'agregations', 'u', 'have', 'done', 'in', 'your', 'project?'],\n ['4)have',\n  'you',\n  'worked',\n  'on',\n  'any',\n  'pyspark',\n  'performance',\n  'tuning',\n  'jobs?'],\n ['5)what', 'was', 'the', 'purpose', 'of', 'doing', 'partition?'],\n ['6)Why', 'partition', 'is', 'it', 'due', 'to', 'skewness?'],\n ['7)have', 'u', 'used', 'repartition', 'and', 'Coalesce?'],\n ['8)have',\n  'you',\n  'used',\n  'AWS',\n  'only',\n  'for',\n  'storage',\n  'or',\n  'analytical?'],\n ['9)Redshift',\n  'is',\n  'used',\n  'for',\n  'sql',\n  'quires',\n  'like',\n  'datawarehouse?'],\n [''],\n [''],\n ['1)what', 'is', 'difference', 'between', 'hospitalaty', 'and', 'hospital'],\n ['2)what',\n  'is',\n  'source',\n  'of',\n  'data',\n  'and',\n  'destination',\n  'of',\n  'data',\n  'keeping'],\n ['3)how', 'to', 'read', 'data', 'in', 'spark?'],\n ['4)how', 'to', 'read', 'data', 'in', 's3', 'to', 'in', 'python', 'file'],\n ['5)can', 'you', 'write', 'spark', 'session'],\n ['6)what',\n  'is',\n  'transformation',\n  'you',\n  'performed?',\n  'and',\n  'write',\n  'syntax'],\n ['7)how', 'will', 'you', 'do', 'transformation', 'in', 'dataframe', '?'],\n ['8)while',\n  'map',\n  'transormations',\n  'using',\n  'how',\n  'you',\n  'print',\n  'paritcular',\n  'column?'],\n ['9)how',\n  'will',\n  'do',\n  'map',\n  'function',\n  'in',\n  'rdd',\n  'and',\n  'dataframe',\n  'explain?'],\n ['10)how', 'you', 'used', 'map', 'functions', 'in', 'your', 'project?'],\n ['11)why',\n  'rdd',\n  'instead',\n  'of',\n  'dataframe',\n  'using',\n  'explain',\n  'in',\n  'your',\n  'project'],\n ['12)write',\n  'sql',\n  'quire',\n  'using',\n  'emp_id,first_name,last_name,manager_id,salary,',\n  'whose',\n  'salary',\n  'is',\n  'greater',\n  'than',\n  'the',\n  'manager',\n  'salary?']]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[9]: [['1)Self', 'intro'],\n ['2)what',\n  'are',\n  'the',\n  'transformations',\n  'and',\n  'where',\n  'did',\n  'u',\n  'used',\n  'these',\n  'transformations',\n  'that',\n  'u',\n  'have',\n  'used',\n  'in',\n  'the',\n  'project'],\n ['3)any', 'agregations', 'u', 'have', 'done', 'in', 'your', 'project?'],\n ['4)have',\n  'you',\n  'worked',\n  'on',\n  'any',\n  'pyspark',\n  'performance',\n  'tuning',\n  'jobs?'],\n ['5)what', 'was', 'the', 'purpose', 'of', 'doing', 'partition?'],\n ['6)Why', 'partition', 'is', 'it', 'due', 'to', 'skewness?'],\n ['7)have', 'u', 'used', 'repartition', 'and', 'Coalesce?'],\n ['8)have',\n  'you',\n  'used',\n  'AWS',\n  'only',\n  'for',\n  'storage',\n  'or',\n  'analytical?'],\n ['9)Redshift',\n  'is',\n  'used',\n  'for',\n  'sql',\n  'quires',\n  'like',\n  'datawarehouse?'],\n [''],\n [''],\n ['1)what', 'is', 'difference', 'between', 'hospitalaty', 'and', 'hospital'],\n ['2)what',\n  'is',\n  'source',\n  'of',\n  'data',\n  'and',\n  'destination',\n  'of',\n  'data',\n  'keeping'],\n ['3)how', 'to', 'read', 'data', 'in', 'spark?'],\n ['4)how', 'to', 'read', 'data', 'in', 's3', 'to', 'in', 'python', 'file'],\n ['5)can', 'you', 'write', 'spark', 'session'],\n ['6)what',\n  'is',\n  'transformation',\n  'you',\n  'performed?',\n  'and',\n  'write',\n  'syntax'],\n ['7)how', 'will', 'you', 'do', 'transformation', 'in', 'dataframe', '?'],\n ['8)while',\n  'map',\n  'transormations',\n  'using',\n  'how',\n  'you',\n  'print',\n  'paritcular',\n  'column?'],\n ['9)how',\n  'will',\n  'do',\n  'map',\n  'function',\n  'in',\n  'rdd',\n  'and',\n  'dataframe',\n  'explain?'],\n ['10)how', 'you', 'used', 'map', 'functions', 'in', 'your', 'project?'],\n ['11)why',\n  'rdd',\n  'instead',\n  'of',\n  'dataframe',\n  'using',\n  'explain',\n  'in',\n  'your',\n  'project'],\n ['12)write',\n  'sql',\n  'quire',\n  'using',\n  'emp_id,first_name,last_name,manager_id,salary,',\n  'whose',\n  'salary',\n  'is',\n  'greater',\n  'than',\n  'the',\n  'manager',\n  'salary?']]"]}}],"execution_count":0},{"cell_type":"code","source":["flatmapped = df.flatMap(lambda x: x.split(\" \"))\nflatmapped.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1211f8b-f72a-4cd6-84f4-065030af490b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[10]: ['1)Self',\n 'intro',\n '2)what',\n 'are',\n 'the',\n 'transformations',\n 'and',\n 'where',\n 'did',\n 'u',\n 'used',\n 'these',\n 'transformations',\n 'that',\n 'u',\n 'have',\n 'used',\n 'in',\n 'the',\n 'project',\n '3)any',\n 'agregations',\n 'u',\n 'have',\n 'done',\n 'in',\n 'your',\n 'project?',\n '4)have',\n 'you',\n 'worked',\n 'on',\n 'any',\n 'pyspark',\n 'performance',\n 'tuning',\n 'jobs?',\n '5)what',\n 'was',\n 'the',\n 'purpose',\n 'of',\n 'doing',\n 'partition?',\n '6)Why',\n 'partition',\n 'is',\n 'it',\n 'due',\n 'to',\n 'skewness?',\n '7)have',\n 'u',\n 'used',\n 'repartition',\n 'and',\n 'Coalesce?',\n '8)have',\n 'you',\n 'used',\n 'AWS',\n 'only',\n 'for',\n 'storage',\n 'or',\n 'analytical?',\n '9)Redshift',\n 'is',\n 'used',\n 'for',\n 'sql',\n 'quires',\n 'like',\n 'datawarehouse?',\n '',\n '',\n '1)what',\n 'is',\n 'difference',\n 'between',\n 'hospitalaty',\n 'and',\n 'hospital',\n '2)what',\n 'is',\n 'source',\n 'of',\n 'data',\n 'and',\n 'destination',\n 'of',\n 'data',\n 'keeping',\n '3)how',\n 'to',\n 'read',\n 'data',\n 'in',\n 'spark?',\n '4)how',\n 'to',\n 'read',\n 'data',\n 'in',\n 's3',\n 'to',\n 'in',\n 'python',\n 'file',\n '5)can',\n 'you',\n 'write',\n 'spark',\n 'session',\n '6)what',\n 'is',\n 'transformation',\n 'you',\n 'performed?',\n 'and',\n 'write',\n 'syntax',\n '7)how',\n 'will',\n 'you',\n 'do',\n 'transformation',\n 'in',\n 'dataframe',\n '?',\n '8)while',\n 'map',\n 'transormations',\n 'using',\n 'how',\n 'you',\n 'print',\n 'paritcular',\n 'column?',\n '9)how',\n 'will',\n 'do',\n 'map',\n 'function',\n 'in',\n 'rdd',\n 'and',\n 'dataframe',\n 'explain?',\n '10)how',\n 'you',\n 'used',\n 'map',\n 'functions',\n 'in',\n 'your',\n 'project?',\n '11)why',\n 'rdd',\n 'instead',\n 'of',\n 'dataframe',\n 'using',\n 'explain',\n 'in',\n 'your',\n 'project',\n '12)write',\n 'sql',\n 'quire',\n 'using',\n 'emp_id,first_name,last_name,manager_id,salary,',\n 'whose',\n 'salary',\n 'is',\n 'greater',\n 'than',\n 'the',\n 'manager',\n 'salary?']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[10]: ['1)Self',\n 'intro',\n '2)what',\n 'are',\n 'the',\n 'transformations',\n 'and',\n 'where',\n 'did',\n 'u',\n 'used',\n 'these',\n 'transformations',\n 'that',\n 'u',\n 'have',\n 'used',\n 'in',\n 'the',\n 'project',\n '3)any',\n 'agregations',\n 'u',\n 'have',\n 'done',\n 'in',\n 'your',\n 'project?',\n '4)have',\n 'you',\n 'worked',\n 'on',\n 'any',\n 'pyspark',\n 'performance',\n 'tuning',\n 'jobs?',\n '5)what',\n 'was',\n 'the',\n 'purpose',\n 'of',\n 'doing',\n 'partition?',\n '6)Why',\n 'partition',\n 'is',\n 'it',\n 'due',\n 'to',\n 'skewness?',\n '7)have',\n 'u',\n 'used',\n 'repartition',\n 'and',\n 'Coalesce?',\n '8)have',\n 'you',\n 'used',\n 'AWS',\n 'only',\n 'for',\n 'storage',\n 'or',\n 'analytical?',\n '9)Redshift',\n 'is',\n 'used',\n 'for',\n 'sql',\n 'quires',\n 'like',\n 'datawarehouse?',\n '',\n '',\n '1)what',\n 'is',\n 'difference',\n 'between',\n 'hospitalaty',\n 'and',\n 'hospital',\n '2)what',\n 'is',\n 'source',\n 'of',\n 'data',\n 'and',\n 'destination',\n 'of',\n 'data',\n 'keeping',\n '3)how',\n 'to',\n 'read',\n 'data',\n 'in',\n 'spark?',\n '4)how',\n 'to',\n 'read',\n 'data',\n 'in',\n 's3',\n 'to',\n 'in',\n 'python',\n 'file',\n '5)can',\n 'you',\n 'write',\n 'spark',\n 'session',\n '6)what',\n 'is',\n 'transformation',\n 'you',\n 'performed?',\n 'and',\n 'write',\n 'syntax',\n '7)how',\n 'will',\n 'you',\n 'do',\n 'transformation',\n 'in',\n 'dataframe',\n '?',\n '8)while',\n 'map',\n 'transormations',\n 'using',\n 'how',\n 'you',\n 'print',\n 'paritcular',\n 'column?',\n '9)how',\n 'will',\n 'do',\n 'map',\n 'function',\n 'in',\n 'rdd',\n 'and',\n 'dataframe',\n 'explain?',\n '10)how',\n 'you',\n 'used',\n 'map',\n 'functions',\n 'in',\n 'your',\n 'project?',\n '11)why',\n 'rdd',\n 'instead',\n 'of',\n 'dataframe',\n 'using',\n 'explain',\n 'in',\n 'your',\n 'project',\n '12)write',\n 'sql',\n 'quire',\n 'using',\n 'emp_id,first_name,last_name,manager_id,salary,',\n 'whose',\n 'salary',\n 'is',\n 'greater',\n 'than',\n 'the',\n 'manager',\n 'salary?']"]}}],"execution_count":0},{"cell_type":"code","source":["x = df.filter(lambda x:x!= \"and\")\nx.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b444de75-69a4-44bd-a0c4-766e86cbeb4b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: ['1)Self intro',\n '2)what are the transformations and where did u used these transformations that u have used in the project',\n '3)any agregations u have done in your project?',\n '4)have you worked on any pyspark performance tuning jobs?',\n '5)what was the purpose of doing partition?',\n '6)Why partition is it due to skewness?',\n '7)have u used repartition and Coalesce?',\n '8)have you used AWS only for storage or analytical?',\n '9)Redshift is used for sql quires like datawarehouse?',\n '',\n '',\n '1)what is difference between hospitalaty and hospital',\n '2)what is source of data and destination of data keeping',\n '3)how to read data in spark?',\n '4)how to read data in s3 to in python file',\n '5)can you write spark session',\n '6)what is transformation you performed? and write syntax',\n '7)how will you do transformation in dataframe ?',\n '8)while map transormations using how you print paritcular column?',\n '9)how will do map function in rdd and dataframe explain?',\n '10)how you used map functions in your project?',\n '11)why rdd instead of dataframe using explain in your project',\n '12)write sql quire using emp_id,first_name,last_name,manager_id,salary, whose salary is greater than the manager salary?']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: ['1)Self intro',\n '2)what are the transformations and where did u used these transformations that u have used in the project',\n '3)any agregations u have done in your project?',\n '4)have you worked on any pyspark performance tuning jobs?',\n '5)what was the purpose of doing partition?',\n '6)Why partition is it due to skewness?',\n '7)have u used repartition and Coalesce?',\n '8)have you used AWS only for storage or analytical?',\n '9)Redshift is used for sql quires like datawarehouse?',\n '',\n '',\n '1)what is difference between hospitalaty and hospital',\n '2)what is source of data and destination of data keeping',\n '3)how to read data in spark?',\n '4)how to read data in s3 to in python file',\n '5)can you write spark session',\n '6)what is transformation you performed? and write syntax',\n '7)how will you do transformation in dataframe ?',\n '8)while map transormations using how you print paritcular column?',\n '9)how will do map function in rdd and dataframe explain?',\n '10)how you used map functions in your project?',\n '11)why rdd instead of dataframe using explain in your project',\n '12)write sql quire using emp_id,first_name,last_name,manager_id,salary, whose salary is greater than the manager salary?']"]}}],"execution_count":0},{"cell_type":"code","source":["dfs = sc.textFile(\"/FileStore/tables/quiz.txt\")\ndfs.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12e3a957-677d-4a23-9879-639d071f0aa9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: ['this mango company animal',\n 'cat dog ant mic laptop',\n 'chair switch mobile am charger cover',\n 'amanda any alarm ant']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: ['this mango company animal',\n 'cat dog ant mic laptop',\n 'chair switch mobile am charger cover',\n 'amanda any alarm ant']"]}}],"execution_count":0},{"cell_type":"code","source":["flatMappedRdd = dfs.flatMap(lambda x: x.split(' '))\n\ndef filterAandC(x):\n  if x.startswith('a') or x.startswith('c'):\n    return False\n  else:\n    return True\n\nfilteredRdd = flatMappedRdd.filter(filterAandC)\n\nfilteredRdd.collect()\n\nfilteredRddLambda = flatMappedRdd.filter(lambda x: not (x.startswith('a') or x.startswith('c')) )\n\nfilteredRddLambda.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0e71be3-dd82-4a20-8df6-623b00b8ba20"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[13]: ['this', 'mango', 'dog', 'mic', 'laptop', 'switch', 'mobile']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[13]: ['this', 'mango', 'dog', 'mic', 'laptop', 'switch', 'mobile']"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = dfs.distinct()\nrdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"982397e0-dc7b-4f08-983f-fd3939cbe060"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[14]: ['cat dog ant mic laptop',\n 'chair switch mobile am charger cover',\n 'this mango company animal',\n 'amanda any alarm ant']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[14]: ['cat dog ant mic laptop',\n 'chair switch mobile am charger cover',\n 'this mango company animal',\n 'amanda any alarm ant']"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"046114d7-43d1-46a7-9067-8787764695e0"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Map Function","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":997522487238736}},"nbformat":4,"nbformat_minor":0}
